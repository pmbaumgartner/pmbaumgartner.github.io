<!DOCTYPE html>
<html lang='en'>

<head>
  <meta charset='utf-8'>
<meta name='viewport' content='width=device-width, initial-scale=1'>
<meta name='description' content='What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.
For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.
I&rsquo;ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.
What are they good for?'>
<meta name='theme-color' content='#297FD5'>

<meta property='og:title' content='Word Embeddings Explainer • Peter Baumgartner'>
<meta property='og:description' content='What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.
For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.
I&rsquo;ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.
What are they good for?'>
<meta property='og:url' content='https://pmbaumgartner.github.io/blog/word-embeddings-explainer/'>
<meta property='og:site_name' content='Peter Baumgartner'>
<meta property='og:type' content='article'><meta property='article:section' content='Blog'><meta property='article:published_time' content='2018-04-30T15:22:07-04:00'/><meta property='article:modified_time' content='2018-04-30T15:22:07-04:00'/><meta name='twitter:card' content='summary'>

<meta name="generator" content="Hugo 0.38.1" />

  <title>Word Embeddings Explainer • Peter Baumgartner</title>
  <link rel='canonical' href='https://pmbaumgartner.github.io/blog/word-embeddings-explainer/'>
  
  
  <link rel='icon' href='/favicon.ico'>
<link rel='stylesheet' href='/assets/css/main.b8f9eb89.css'><link rel='stylesheet' href='/css/custom.css'><style>
:root{--color-accent:#297FD5;}
</style>

<script>
window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
ga('create', 'UA-72692144-1', 'auto');
ga('send', 'pageview');
</script>
<script async src='//www.google-analytics.com/analytics.js'></script>

</head>


<body class='page type-blog has-sidebar'>

  <div class='site'>

    <div id='sidebar' class='sidebar'>
  <a class='screen-reader-text' href='#main-menu'>Skip to Main Menu</a>

  <div class='container'><section class='widget widget-about sep-after'>
  <header>
    
    <div class='logo'>
      <a href='/'>
        <img src='/images/LAZERS.jpg'>
      </a>
    </div>
    
    <h2 class='title site-title '>
    Peter Baumgartner
    </h2>
    <div class='desc'>
    Data Scientist
    </div>
  </header>

</section>
</div>

  <div class='sidebar-overlay'></div>
</div>

    <div class='main'>

      <nav id='main-menu' class='main-menu' aria-label='Main Menu'>
  <div class='container'>
    <a class='screen-reader-text' href='#content'>Skip to Content</a>

<button id='sidebar-toggler' class='sidebar-toggler' aria-controls='sidebar'>
  <span class='screen-reader-text'>Toggle Sidebar</span>
  <span class='open'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="3" y1="12" x2="21" y2="12" />
  <line x1="3" y1="6" x2="21" y2="6" />
  <line x1="3" y1="18" x2="21" y2="18" />
  
</svg>
</span>
  <span class='close'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="18" y1="6" x2="6" y2="18" />
  <line x1="6" y1="6" x2="18" y2="18" />
  
</svg>
</span>
</button>
    <ul><li class='item'>
        <a href='/'>Home</a>
      </li><li class='item'>
        <a href='/blog/'>Blog</a>
      </li><li class='item'>
        <a href='/notebooks/'>Notebooks</a>
      </li><li class='item'>
        <a href='/now/'>Now</a>
      </li></ul>
  </div>
</nav>


      <header id='header' class='header site-header'>
        <div class='container sep-after'>
          <div class='header-info'><p class='site-title title'>Peter Baumgartner</p><p class='desc site-desc'></p>
          </div>
        </div>
      </header>

      <main id='content'>


<article lang='en' class='entry'>
  <header class='header entry-header'>
  <div class='container sep-after'>
    <div class='header-info'>
      <h1 class='title'>Word Embeddings Explainer</h1>
      

    </div>
    
<div class='entry-meta'>
  <span class='posted-on'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <rect x="3" y="4" width="18" height="18" rx="2" ry="2"/>
  <line x1="16" y1="2" x2="16" y2="6"/>
  <line x1="8" y1="2" x2="8" y2="6"/>
  <line x1="3" y1="10" x2="21" y2="10"/>
  
</svg>
<span class='screen-reader-text'>Posted on </span>
  <time class='entry-date' datetime='2018-04-30T15:22:07-04:00'>2018-04-30</time>
</span>

  
  
<span class='reading-time'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <circle cx="12" cy="12" r="10"/>
  <polyline points="12 6 12 12 15 15"/>
  
</svg>
4 mins read
</span>


</div>


  </div>
</header>

  
  

  <div class='container entry-content'>
  

<h3 id="what-are-word-embeddings">What are word embeddings?</h3>

<p>Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.</p>

<p>For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.</p>

<p><img src="/images/wordville.png" alt="Wordville" /></p>

<p>I&rsquo;ve drawn 3 <em>neighborhoods</em> over this embedding to illustrate the semantic groupings.</p>

<h3 id="what-are-they-good-for">What are they good for?</h3>

<p>A word embedding model transforms words into numbers so that we can do interesting measurements with them. Below are some applications of word embeddings.</p>

<h4 id="word-similarity">Word Similarity</h4>

<p>You can ask a word who its neighbors are. With enough words, this similarity query can work like a synonym finder. In the 2D grid representation above, each immediate neighbor is equally distant from some central term, but with the full dimension word embeddings you can get a more nuanced similarity score for each word.</p>

<h4 id="sentence-similarity">Sentence Similarity</h4>

<p>Sentences are made of words, so we can calculate a semantic distance between sentences using embeddings. Here&rsquo;s 3 sample sentences, with words we have addresses for in bold.</p>

<ol>
<li>There&rsquo;s a <strong>tall</strong> <strong>patch</strong> of <strong>grass</strong>.</li>
<li>The <strong>root</strong> of the <strong>tree</strong> is in <strong>soil</strong>.</li>
<li>Let&rsquo;s <strong>chat</strong> about <strong>pizza</strong> and <strong>cake</strong>!</li>
</ol>

<p>It&rsquo;s clear to us, as humans, that the first two sentences share meaning and the 3rd sentence is different than them. A common way of calculating the similarity of two pieces of text is to count how many words overlap — however,  none of the sentences above have any meaningful words in common (once we remove common <em><a href="https://en.wikipedia.org/wiki/Stop_words">stopwords</a></em> like &ldquo;the&rdquo;). When measuring similarity in this way, all three of our example sentences are equally similar, since they all don&rsquo;t share any words. With word embeddings, we can use semantic distance for this case.</p>

<h5 id="measuring-semantic-similarity-with-embeddings">Measuring semantic similarity with embeddings</h5>

<p>With word embeddings there are a few ways to measure the similarity of two sentences.</p>

<ol>
<li><strong>Average Embeddings</strong> - Find the <em>average</em> location (centroid) of the words in both sentences. Then measure the distance between these two <em>average</em> locations.</li>
<li><strong>Word Movers Distance</strong> - Find the total cost of moving from all the words in one sentence to all the words in another sentence.</li>
<li><strong>Soft Cosine Similarity</strong> - Combine the magic of linear algebra and the location of your word embeddings to create a new space to measure similarity. (There&rsquo;s a lot behind this one, but you should know that its fast and it <a href="http://www.redalyc.org/html/615/61532067007/">works well</a>.)</li>
</ol>

<h3 id="what-s-the-difference-between-this-simplification-and-actual-word-embeddings">What&rsquo;s the difference between this simplification and actual word embeddings?</h3>

<p>Word embeddings are usually more than 2 dimensions (commonly 50, 100, 200, and 300). For this example, I used the 300D vectors trained on Google News. That high dimensionality is helpful for solving large computational problems, and each dimension captures some unique semantic structure of the words given text the model was trained on.</p>

<p>To get to the 2D Map of Wordville, I did the following:</p>

<ol>
<li>I used <a href="https://norvig.com/ngrams/">this</a> helpful list of common english words as a source. I looped through that list in order, picking out words that were 4- or 5-letters long (so their label would fit in the map) and were semantically similar (measured with a word embedding model) to some basic seed words (car, dog, food) until I had 625 words.</li>
<li>Projected the vectors of these words down to two dimensions with a technique called <a href="https://github.com/lmcinnes/umap">Universal Manifold Approximation and Projection</a> (UMAP).</li>
<li>Used an optimization algorithm to map each point in 2D to a grid of 2D points, following <a href="https://github.com/kylemcdonald/CloudToGrid/blob/master/CloudToGrid.ipynb">this notebook</a>.</li>
</ol>

<h3 id="how-can-i-make-my-own-word-embeddings">How can I make my own word embeddings?</h3>

<p>The <a href="https://github.com/RaRe-Technologies/gensim">gensim</a> library in python allows you to create your own word embeddings. It requires your input text to be tokenized.</p>

<p>Here&rsquo;s a basic example &ndash; we will create a model from 10000 copies of 4 tokenized sentences. (Note: You will not get good results with this input data.)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">import</span> gensim 

tokenized_sentences <span style="color:#f92672">=</span> [
    [<span style="color:#e6db74">&#39;I&#39;</span>, <span style="color:#e6db74">&#39;love&#39;</span>, <span style="color:#e6db74">&#39;my&#39;</span>, <span style="color:#e6db74">&#39;cat&#39;</span>, <span style="color:#e6db74">&#39;.&#39;</span>],
    [<span style="color:#e6db74">&#39;I&#39;</span>, <span style="color:#e6db74">&#39;love&#39;</span>, <span style="color:#e6db74">&#39;my&#39;</span>, <span style="color:#e6db74">&#39;dog&#39;</span>, <span style="color:#e6db74">&#39;!&#39;</span>],
    [<span style="color:#e6db74">&#39;I&#39;</span>, <span style="color:#e6db74">&#39;love&#39;</span>, <span style="color:#e6db74">&#39;my&#39;</span>, <span style="color:#e6db74">&#39;cat&#39;</span>, <span style="color:#e6db74">&#39;,&#39;</span>, <span style="color:#e6db74">&#39;sometimes&#39;</span>, <span style="color:#e6db74">&#39;.&#39;</span>],
    [<span style="color:#e6db74">&#39;I&#39;</span>, <span style="color:#e6db74">&#39;also&#39;</span>, <span style="color:#e6db74">&#39;like&#39;</span>, <span style="color:#e6db74">&#39;my&#39;</span>, <span style="color:#e6db74">&#39;dog&#39;</span>, <span style="color:#e6db74">&#39;!&#39;</span>]
]

many_tokenized_sentences <span style="color:#f92672">=</span> tokenized_sentences <span style="color:#f92672">*</span> <span style="color:#ae81ff">10000</span>

<span style="color:#75715e"># create the model</span>
model <span style="color:#f92672">=</span> gensim<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>Word2Vec(sentences<span style="color:#f92672">=</span>many_tokenized_sentences)

<span style="color:#75715e"># find similar words</span>
model<span style="color:#f92672">.</span>wv<span style="color:#f92672">.</span>most_similar(<span style="color:#e6db74">&#39;cat&#39;</span>)</code></pre></div>
<p>One thing to note is that in comparison to other common NLP tasks, you will not want to remove stopwords or punctuation from the vocabulary, since they play an important role in providing context.</p>

</div>

  
<footer class='entry-footer'>
  <div class='container sep-before'>
  
  </div>
</footer>


</article>

<nav class='entry-nav'>
  <div class='container'><div class='prev-entry sep-before'>
      <a href='/blog/testing-ipython-magics/'>
        <span aria-hidden='true'><svg class='icon' viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
  
  <line x1="20" y1="12" x2="4" y2="12"/>
  <polyline points="10 18 4 12 10 6"/>
  
</svg>
 Previous</span>
        <span class='screen-reader-text'>Previous post: </span>How to Test IPython Magic</a>
    </div></div>
</nav>




      </main>

      <footer id='footer' class='footer'>
        <div class='container sep-before'>
          
          <div class='copyright'>
  <p></p>
</div>

        </div>
      </footer>

    </div>
  </div><script>window.__public_path__='\/assets\/js\/'</script>

<script src='/assets/js/main.4d3afaaf.js'></script><script src='/js/custom.js'></script>
</body>

</html>

