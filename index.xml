<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Peter Baumgartner</title>
    <link>https://pmbaumgartner.github.io/</link>
    <description>Recent content in Home on Peter Baumgartner</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 06 Jul 2019 06:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://pmbaumgartner.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Applied NLP: Lessons from the Field</title>
      <link>https://pmbaumgartner.github.io/blog/applied-nlp-lessons/</link>
      <pubDate>Sat, 06 Jul 2019 06:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/applied-nlp-lessons/</guid>
      <description>These are the main points and resources for my talk, Applied NLP: Lessons from the Field, delivered at spaCy IRL 2019.
Summary Natural Language Processing projects often fail in their conception, in their delivery, or in their impact. To identify good candidate problems for NLP, talk to the client presenting you with a problem and first discuss how you would solve the problem without NLP. To successfully deliver an NLP project, acknowledge that project management is a skill and learn how to do it well: communicate the uncertainty you face and come up with metaphors to explain your work to non-technical stakeholders.</description>
    </item>
    
    <item>
      <title>Fine-Tuning GPT-2 Small for Generative Text</title>
      <link>https://pmbaumgartner.github.io/blog/gpt2-jokes/</link>
      <pubDate>Sat, 23 Mar 2019 11:16:39 -0400</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/gpt2-jokes/</guid>
      <description>Why did the chicken cross the road? Because it had no legs.
 These are the types of hilarious jokes the gpt-2 small model can generate for you.
After reading a few blog posts here and here, and playing around with gpt-2 small myself, I thought I would write up the full process I used to fine-tune and produce generative text.
For this example, we&amp;rsquo;ll use a dataset of jokes pulled from the /r/jokes subreddit to fine tune the GPT-2 small model to generate new jokes.</description>
    </item>
    
    <item>
      <title>Potential Issues with Criminal Justice Data</title>
      <link>https://pmbaumgartner.github.io/blog/issues-with-criminal-justice-data/</link>
      <pubDate>Mon, 06 Aug 2018 12:40:29 -0400</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/issues-with-criminal-justice-data/</guid>
      <description>Summary: I wrote this document based on my experience validating a risk assessment instrument. These were some of the issues, rewritten for generalizability, that we encountered. Accessible, quality data is often a project bottleneck, and I&amp;rsquo;ve found these helpful to consider before working on a project with criminal justice data.
Comments: I wrote these notes in the context of evaluating a risk assessment instrument, so &amp;ldquo;risk factors&amp;rdquo; are independent variables and &amp;ldquo;outcomes&amp;rdquo; are dependent variables.</description>
    </item>
    
    <item>
      <title>Notes on NLP Projects</title>
      <link>https://pmbaumgartner.github.io/blog/notes-on-nlp-projects/</link>
      <pubDate>Mon, 06 Aug 2018 10:59:48 -0400</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/notes-on-nlp-projects/</guid>
      <description>Summary: These are some notes, combined with my own experience and commentary, derived from Matthew Honnibal&amp;rsquo;s PyData Berlin 2018 talk: Building new NLP solutions with spaCy and Prodigy. I intended to use these as a reference when starting new NLP projects.
In NLP and ML we talk a lot about models and optimization. But this isn&amp;#39;t where the battle is really won! I&amp;#39;ve been trying to explain my thoughts on this lately.</description>
    </item>
    
    <item>
      <title>Holy NLP! Understanding Part of Speech Tags, Dependency Parsing, and Named Entity Recognition</title>
      <link>https://pmbaumgartner.github.io/notebooks/holy-nlp/</link>
      <pubDate>Sat, 19 May 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/notebooks/holy-nlp/</guid>
      <description>Links Link to Notebook

What&amp;rsquo;s in this notebook? This is the notebook behind my blog post Holy NLP! Understanding Part of Speech Tags, Dependency Parsing, and Named Entity Recognition. It&amp;rsquo;s an exploration into three common NLP tasks applied on the Bible as a corpus, with some pandas aggregations and seaborn plotting to round things out.</description>
    </item>
    
    <item>
      <title>Holy NLP! Understanding Part of Speech Tags, Dependency Parsing, and Named Entity Recognition</title>
      <link>https://pmbaumgartner.github.io/blog/holy-nlp/</link>
      <pubDate>Thu, 17 May 2018 19:29:07 -0600</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/holy-nlp/</guid>
      <description>Introduction When we think of data science, we often think of statistical analysis of numbers. But, more and more frequently, organizations generate a lot of unstructured text data that can be quantified and analyzed. A few examples are social network comments, product reviews, emails, interview transcripts.
For analyzing text, data scientists often use Natural Language Processing (NLP). In this blog post we&amp;rsquo;ll 3 we&amp;rsquo;ll walk through 3 common NLP tasks and look at how they can be used together to analyze text.</description>
    </item>
    
    <item>
      <title>Word Embeddings Explainer</title>
      <link>https://pmbaumgartner.github.io/blog/word-embeddings-explainer/</link>
      <pubDate>Mon, 30 Apr 2018 15:22:07 -0400</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/word-embeddings-explainer/</guid>
      <description>What are word embeddings? Imagine if every word had an address you could look up in an address book. Now also imagine if words that shared meaning lived in the same neighborhood. This is a simplified metaphor for word embeddings.
For a visual example, here are simplified word embeddings for common 4- and 5-letter english words.
I&amp;rsquo;ve drawn 3 neighborhoods over this embedding to illustrate the semantic groupings.
What are they good for?</description>
    </item>
    
    <item>
      <title>How to Test IPython Magic</title>
      <link>https://pmbaumgartner.github.io/blog/testing-ipython-magics/</link>
      <pubDate>Fri, 16 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/testing-ipython-magics/</guid>
      <description>TL;DR If you want to test ipython magics you can do the following:
 Import the global ipython app with from IPython.testing.globalipapp import get_ipython Crete an object with the global ipython app with ip = get_ipython() Load your magic with ip.magic(&#39;load_ext your_magic_name&#39;) Run your magic with ip.run_line_magic(&#39;your_magic_function&#39;, &#39;your_magic_arguments&#39;) (Optional) Access results of your magic with ip.user_ns (ipython user namespace).  An example test using pytest looks like this:
import pytest from IPython.</description>
    </item>
    
    <item>
      <title>An Exploration in Earth &amp; Word Movers Distance</title>
      <link>https://pmbaumgartner.github.io/blog/word-movers-distance-exploration/</link>
      <pubDate>Sun, 18 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/word-movers-distance-exploration/</guid>
      <description>This post will be an exploration into Earth Mover&amp;rsquo;s Distance as well as its application to NLP problems through Word Movers Distance.
To get started, we&amp;rsquo;ll follow the benign pedagogical path of copying the Wikipedia definition: &amp;gt; The earth mover&amp;rsquo;s distance (EMD) is a measure of the distance between two probability distributions over a region D. In mathematics, this is known as the Wasserstein metric. Informally, if the distributions are interpreted as two different ways of piling up a certain amount of dirt over the region D, the EMD is the minimum cost of turning one pile into the other; where the cost is assumed to be amount of dirt moved times the distance by which it is moved.</description>
    </item>
    
    <item>
      <title>An Exploration in Earth &amp; Word Movers Distance</title>
      <link>https://pmbaumgartner.github.io/notebooks/word-movers-distance-exploration/</link>
      <pubDate>Sun, 18 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/notebooks/word-movers-distance-exploration/</guid>
      <description>Links Link to Notebook

What&amp;rsquo;s in this notebook? This is the notebook behind my blog post An Exploration in Earth &amp;amp; Word Movers Distance. It&amp;rsquo;s an exploration into Earth/Word Movers Distance algorithm that includes a there&amp;rsquo;s a lot of great matplotlib plots and some pandas-fu.</description>
    </item>
    
    <item>
      <title>The Impact of Model Output Transformations on ROC</title>
      <link>https://pmbaumgartner.github.io/blog/output-transformations-roc-impact/</link>
      <pubDate>Sun, 21 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/output-transformations-roc-impact/</guid>
      <description>Risk Assessment tools are currently used to assist in decision-making at several points in the criminal justice system. These tools take in some data about an individual and to provide a &amp;lsquo;risk score&amp;rsquo; for an individual that&amp;rsquo;s reflective of their likelihood of committing a specific behavior in the future. A standard outcome of interest is recidivism, or a person&#39;s relapse into criminal behavior, often after the person receives sanctions or undergoes intervention for a previous crime (NIJ).</description>
    </item>
    
    <item>
      <title>The Impact of Model Output Transformations on ROC</title>
      <link>https://pmbaumgartner.github.io/notebooks/output-transformations-roc-impact/</link>
      <pubDate>Sun, 21 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/notebooks/output-transformations-roc-impact/</guid>
      <description>Links Link to Notebook

What&amp;rsquo;s in this notebook? This is the notebook behind my blog post The Impact of Model Output Transformations on ROC. It contains some seaborn plots, some pandas-fu with method chaining, a simulation of analyzing model results, and some plots with seaborn.</description>
    </item>
    
    <item>
      <title>Colored ROC Curves</title>
      <link>https://pmbaumgartner.github.io/notebooks/colored-roc-curves/</link>
      <pubDate>Tue, 04 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/notebooks/colored-roc-curves/</guid>
      <description>Links Link to Notebook

What&amp;rsquo;s in this notebook? The Receiver Operating Characteristic (ROC) curve is helpful in evaluating model performance, especially since Area Under the Curve (AUC ROC) has a several friendly interpretations. I use ROC curves in evaluating models I have to explain the model performance to non-technical folks. I was reading through Machine Learning: The Art and Science of Algorithms that Make Sense of Data and stumbled upon this nice visual and interpretation of ROC (tied to AUC):</description>
    </item>
    
    <item>
      <title>Biclustering (with Paul Revere)</title>
      <link>https://pmbaumgartner.github.io/notebooks/biclustering-with-paul-revere/</link>
      <pubDate>Wed, 29 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/notebooks/biclustering-with-paul-revere/</guid>
      <description>Links Link to Notebook

What&amp;rsquo;s in this notebook? Biclustering is an unsupervised learning algorithm that clusters both the rows and columns. This notebook contains a workflow for completing biclustering in a network analysis context as well as a clean export of the data to an excel spreadsheet.</description>
    </item>
    
    <item>
      <title>Dimension Reduction with TSNE and Bokeh Scatterplot</title>
      <link>https://pmbaumgartner.github.io/notebooks/tsne-to-bokeh/</link>
      <pubDate>Sat, 25 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/notebooks/tsne-to-bokeh/</guid>
      <description>Link to Notebook

What&amp;rsquo;s in this notebook? This is a workflow I use often in data exploration. TSNE gives a good representation of high-dimensional data, and Bokeh is helpful in creating a simple interactive plots with contextual info given by colors and tooltips.
This workflow has been extremely helpful for:
 text analytics/NLP tasks if text data is passed through a TfidfVectorizer or similar from scikit-learn understanding word2vec or doc2vec vectors by passing them to TSNE getting an idea of separability in doing prediction / classification by passing the outcome variable to bokeh  This example uses the Australian atheletes data set, which contains 11 numeric variables.</description>
    </item>
    
    <item>
      <title>Some Tips for Using Jupyter Notebooks with Pelican</title>
      <link>https://pmbaumgartner.github.io/blog/jupyter-notebooks-for-pelican/</link>
      <pubDate>Fri, 17 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/jupyter-notebooks-for-pelican/</guid>
      <description>Note (2018-04-09): I no longer use Pelican as the engine to build my blog, so you want be able to see parts of this workflow in this blog&amp;rsquo;s repository. The insipiration for ideas in this post is captured in this notebook from Chris Albon. Switching static site generators is a great way to kill a few hours on the weekend. I was previously using Jekyll because it works seamlessly with Github Pages, but I&amp;rsquo;m a python person so I figured I&amp;rsquo;d learn something new and move everything over to Pelican.</description>
    </item>
    
    <item>
      <title>PyData Carolinas Recap &amp; Presentation Reflection</title>
      <link>https://pmbaumgartner.github.io/blog/pydata-carolinas-recap/</link>
      <pubDate>Sat, 17 Sep 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/pydata-carolinas-recap/</guid>
      <description>PyData Carolinas Recap &amp;amp; Presentation Reflection I was fortunate enough this year to attend the first PyData Carolinas conference, though my attendance was only made possible with the development and delivery of a tutorial talk. My colleague Rob and I proposed and then delivered a 90m tutorial talk on using NetworkX to do Social Network Analysis in Python (repo, video coming soon). The talk was aimed at intermediate users of python with some experience with data and the python language.</description>
    </item>
    
    <item>
      <title>Creating Slack Slash Commands with Python and Flask: Part 1</title>
      <link>https://pmbaumgartner.github.io/blog/slack-commands-with-python-and-flask/</link>
      <pubDate>Mon, 18 Jan 2016 00:00:00 +0000</pubDate>
      
      <guid>https://pmbaumgartner.github.io/blog/slack-commands-with-python-and-flask/</guid>
      <description>Note (2018-04-09): Slack&#39;s API has changed since I wrote this article. I also never wrote a part 2. If you want an up to date tutorial this blog from DigitalOcean is good. Part 1: Setting Up Our Workflow and a Simple Application A few weekends ago my pet project was to set up a drive time slash command in Slack. Searching through our organization&amp;rsquo;s Slack conversation history, on top of overhearing several conversations, it seems like traffic is both a source of anguish and a favorite topic for smalltalk in our office.</description>
    </item>
    
  </channel>
</rss>